{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f887af7",
   "metadata": {},
   "source": [
    "### TEXT CLASSIFICATION MODEL\n",
    "\n",
    "Text classification is the process of classifying text strings or documents into different categories, depending upon the contents of the strings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e08db79",
   "metadata": {},
   "source": [
    "### Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c6fe7dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/shashwatiswain/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.datasets import load_files\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f131ca",
   "metadata": {},
   "source": [
    "### Reading the Data\n",
    "\n",
    "Dataset Used: Cornell sentiment analysis dataset(polarity_dataset_v2.0) \n",
    "https://www.cs.cornell.edu/people/pabo/movie-review-data/\n",
    "\n",
    "This dataset contains 1000 positive and 1000 negative reviews.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "966cf391",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = load_files('review_polarity/txt_sentoken/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f300f220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deriving the data and the positive or negative label\n",
    "X, y = reviews.data, reviews.target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0223d49",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b13b3d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "stemmer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8724e569",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for i in range(0, len(X)):\n",
    "    #Removing non-word characters\n",
    "    review = re.sub(r'\\W',' ',str(X[i]))\n",
    "    #Converting all characters to lower case\n",
    "    review = review.lower()\n",
    "    #Removing single characters\n",
    "    review = re.sub(r'\\s+[a-z]\\s+',' ', review)\n",
    "    #Removing single characters at the starting of the sentence\n",
    "    review = re.sub(r'^[a-z]\\s+', ' ', review)\n",
    "    #Removing multiple spaces\n",
    "    review = re.sub(r'\\s+', ' ', review)\n",
    "    # Removing prefixed 'b'\n",
    "    review = re.sub(r'^b\\s+', '', review)\n",
    "    # Lemmatization\n",
    "    review = review.split()\n",
    "    review = [stemmer.lemmatize(word) for word in review]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf079991",
   "metadata": {},
   "source": [
    "### Extracting Features using CountVectorizer (Bag of Words Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6adb9c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "39494185",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(max_features = 3000,\n",
    "                             min_df = 5,   # minimum number of documents that should contain this feature\n",
    "                             max_df = 0.7, # words that occur in a maximum of 70% of all the documents\n",
    "                             stop_words = stopwords.words('english') # to remove the stop words\n",
    "                            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "ef3b3d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "35647d7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 3000)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape # 2000 documents and 4000 features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e461ba62",
   "metadata": {},
   "source": [
    "### TF-IDF Transformation of the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "dbc828ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "81628ec6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 3000)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer = TfidfTransformer()\n",
    "X = transformer.fit_transform(X).toarray()\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d714e1a",
   "metadata": {},
   "source": [
    "### Splitting the dataset into Training and Testing Sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "13ada6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "9a94528c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73513dbf",
   "metadata": {},
   "source": [
    "### Training Text Classification Model and Predicting Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "de0cb6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "353638b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier1 = LogisticRegression()\n",
    "classifier2 = RandomForestClassifier(n_estimators=1000, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "91c2fbf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=1000, random_state=0)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier1.fit(X_train, y_train)\n",
    "classifier2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "30899815",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred1 = classifier1.predict(X_test)\n",
    "y_pred2 = classifier2.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e20fba",
   "metadata": {},
   "source": [
    "### Evaluating the Model on the Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "dd5d6894",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "d855af56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of LR Model 0.82\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[158,  50],\n",
       "       [ 22, 170]])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Accuracy of LR Model\", accuracy_score(y_test, y_pred1))\n",
    "confusion_matrix(y_test, y_pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "49648918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of RF Model 0.8325\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[176,  32],\n",
       "       [ 35, 157]])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Accuracy of RF Model\", accuracy_score(y_test, y_pred2))\n",
    "\n",
    "confusion_matrix(y_test, y_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7999dfd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
